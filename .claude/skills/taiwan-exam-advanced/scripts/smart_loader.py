#!/usr/bin/env python3
"""
æ™ºæ…§çŸ¥è­˜è¼‰å…¥å™¨ - ä¸‰å±¤æ¼¸é€²å¼è¼‰å…¥ç³»çµ±

åŠŸèƒ½ï¼š
1. é¡Œç›®åˆ†é¡ï¼ˆåœ‹æ–‡/ç¤¾æœƒ/åƒ¹å€¼è§€ï¼‰
2. ä¸»é¡Œå®šä½ï¼ˆä½¿ç”¨ topic_index.jsonï¼‰
3. ç²¾ç¢ºæª¢ç´¢ï¼ˆä½¿ç”¨ ripgrepï¼‰
4. æ•ˆèƒ½ç›£æ§èˆ‡å„ªåŒ–å»ºè­°

ä½¿ç”¨ç¯„ä¾‹ï¼š
    python smart_loader.py --question "æç™½çš„è©©æ­Œé¢¨æ ¼ç‚ºä½•ï¼Ÿ"
    python smart_loader.py --topic "äºŒäºŒå…«" --load-details
"""

import json
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime


class SmartLoader:
    """æ™ºæ…§çŸ¥è­˜è¼‰å…¥å™¨"""

    def __init__(self, skill_dir: Optional[str] = None):
        if skill_dir is None:
            # é è¨­è·¯å¾‘ï¼š.claude/skills/taiwan-exam-advanced
            self.skill_dir = Path(__file__).parent.parent
        else:
            self.skill_dir = Path(skill_dir)

        self.topic_index = self._load_json("meta/topic_index.json")
        self.taxonomy = self._load_json("meta/taxonomy.json")
        self.loaded_files = []  # è¨˜éŒ„å·²è¼‰å…¥æª”æ¡ˆ
        self.token_count = 0  # ä¼°ç®— Token æ¶ˆè€—

    def _load_json(self, relative_path: str) -> Dict:
        """è¼‰å…¥ JSON æª”æ¡ˆ"""
        file_path = self.skill_dir / relative_path
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {file_path}", file=sys.stderr)
            return {}

    def classify_question(self, question: str) -> Dict:
        """
        åˆ†é¡é¡Œç›®ä¸¦è¿”å›è¼‰å…¥è·¯å¾‘

        Returns:
            {
                "category": "chinese" | "social" | "values",
                "overview_file": "layer1_overview/chinese_overview.md",
                "confidence": 0.0-1.0,
                "matched_keywords": ["è©©", "è©"]
            }
        """
        keyword_patterns = self.taxonomy.get("keyword_patterns", {})

        results = {}
        for category, keywords in keyword_patterns.items():
            matches = [kw for kw in keywords if kw in question]
            if matches:
                results[category] = {
                    "matches": matches,
                    "count": len(matches),
                }

        if not results:
            return {
                "category": "unknown",
                "overview_file": None,
                "confidence": 0.0,
                "matched_keywords": [],
            }

        # åˆ¤æ–·æœ€å¯èƒ½çš„åˆ†é¡
        if "å¤å…¸æ–‡å­¸æŒ‡æ¨™è©" in results or "ç¾ä»£æ–‡å­¸æŒ‡æ¨™è©" in results or "ä¿®è¾­æŒ‡æ¨™è©" in results:
            category = "chinese"
        elif "æ­·å²æŒ‡æ¨™è©" in results or "åœ°ç†æŒ‡æ¨™è©" in results or "å…¬æ°‘æŒ‡æ¨™è©" in results:
            category = "social"
        elif "åƒ¹å€¼è§€æŒ‡æ¨™è©" in results:
            category = "values"
        else:
            # é¸æ“‡åŒ¹é…æ•¸é‡æœ€å¤šçš„
            category_key = max(results, key=lambda k: results[k]["count"])
            category = category_key.split("æŒ‡æ¨™è©")[0]
            # è™•ç†ä¸­æ–‡å°æ‡‰
            category_mapping = {
                "å¤å…¸æ–‡å­¸": "chinese",
                "ç¾ä»£æ–‡å­¸": "chinese",
                "ä¿®è¾­": "chinese",
                "æ­·å²": "social",
                "åœ°ç†": "social",
                "å…¬æ°‘": "social",
                "åƒ¹å€¼è§€": "values",
            }
            category = category_mapping.get(category, "chinese")

        all_matches = []
        for result in results.values():
            all_matches.extend(result["matches"])

        confidence = min(len(all_matches) / 3, 1.0)  # åŒ¹é… 3 å€‹ä»¥ä¸Šé—œéµå­—è¦–ç‚ºé«˜ä¿¡å¿ƒ

        return {
            "category": category,
            "overview_file": f"layer1_overview/{category}_overview.md",
            "confidence": confidence,
            "matched_keywords": all_matches,
        }

    def find_topics(self, query: str) -> List[Dict]:
        """
        åœ¨ topic_index.json ä¸­æœå°‹ç›¸é—œä¸»é¡Œ

        Returns:
            [
                {
                    "topic": "æç™½",
                    "category": "chinese/classical",
                    "files": ["tangshi.md"],
                    "relevance": 1.0
                }
            ]
        """
        results = []

        for topic, data in self.topic_index.items():
            # ç›´æ¥åŒ¹é…ä¸»é¡Œåç¨±
            if topic in query:
                results.append(
                    {
                        "topic": topic,
                        "category": data["category"],
                        "files": data["files"],
                        "tags": data.get("tags", []),
                        "related": data.get("related", []),
                        "relevance": 1.0,
                    }
                )
                continue

            # åŒ¹é…é—œéµå­—
            keywords = data.get("keywords", [])
            if any(kw in query for kw in keywords):
                results.append(
                    {
                        "topic": topic,
                        "category": data["category"],
                        "files": data["files"],
                        "tags": data.get("tags", []),
                        "related": data.get("related", []),
                        "relevance": 0.8,
                    }
                )

        # æŒ‰ç›¸é—œåº¦æ’åº
        results.sort(key=lambda x: x["relevance"], reverse=True)
        return results

    def search_with_ripgrep(self, query: str, category: str, context_lines: int = 3) -> List[str]:
        """
        ä½¿ç”¨ ripgrep åœ¨ç‰¹å®šåˆ†é¡ä¸­æœå°‹

        Args:
            query: æœå°‹é—œéµå­—
            category: åˆ†é¡ (chinese/social/values)
            context_lines: ä¸Šä¸‹æ–‡è¡Œæ•¸

        Returns:
            æœå°‹çµæœåˆ—è¡¨
        """
        search_dir = self.skill_dir / f"layer2_topics/{category}"

        if not search_dir.exists():
            return []

        try:
            result = subprocess.run(
                ["rg", "-C", str(context_lines), "-i", "--no-heading", query, str(search_dir)],
                capture_output=True,
                text=True,
                timeout=5,
            )

            if result.returncode == 0:
                return result.stdout.strip().split("\n")
            else:
                return []
        except (subprocess.TimeoutExpired, FileNotFoundError):
            # ripgrep æœªå®‰è£æˆ–è¶…æ™‚
            return []

    def load_file(self, relative_path: str, preview_only: bool = False) -> Optional[str]:
        """
        è¼‰å…¥æª”æ¡ˆå…§å®¹

        Args:
            relative_path: ç›¸å°æ–¼ skill_dir çš„è·¯å¾‘
            preview_only: æ˜¯å¦åƒ…è¼‰å…¥å‰ 20 è¡Œ

        Returns:
            æª”æ¡ˆå…§å®¹æˆ– None
        """
        file_path = self.skill_dir / relative_path

        if not file_path.exists():
            return None

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                if preview_only:
                    lines = [f.readline() for _ in range(20)]
                    content = "".join(lines)
                else:
                    content = f.read()

            self.loaded_files.append(relative_path)
            # ä¼°ç®— Tokenï¼ˆç²—ç•¥ï¼š1 token â‰ˆ 4 å­—å…ƒï¼‰
            self.token_count += len(content) // 4

            return content
        except Exception as e:
            print(f"è¼‰å…¥æª”æ¡ˆå¤±æ•— {file_path}: {e}", file=sys.stderr)
            return None

    def get_loading_plan(self, question: str) -> Dict:
        """
        æ ¹æ“šé¡Œç›®ç”Ÿæˆè¼‰å…¥è¨ˆç•«

        Returns:
            {
                "category": "chinese",
                "overview": "layer1_overview/chinese_overview.md",
                "topics": [
                    {
                        "topic": "æç™½",
                        "files": ["layer2_topics/chinese/classical/tangshi.md"]
                    }
                ],
                "estimated_tokens": 8000
            }
        """
        # æ­¥é©Ÿä¸€ï¼šåˆ†é¡
        classification = self.classify_question(question)

        if classification["category"] == "unknown":
            return {
                "category": "unknown",
                "overview": None,
                "topics": [],
                "estimated_tokens": 0,
                "suggestion": "è«‹æä¾›æ›´å…·é«”çš„é—œéµå­—",
            }

        # æ­¥é©ŸäºŒï¼šæ‰¾ä¸»é¡Œ
        topics = self.find_topics(question)

        # æ­¥é©Ÿä¸‰ï¼šç”Ÿæˆè¼‰å…¥è¨ˆç•«
        plan = {
            "category": classification["category"],
            "overview": classification["overview_file"],
            "matched_keywords": classification["matched_keywords"],
            "confidence": classification["confidence"],
            "topics": [],
            "estimated_tokens": 2000,  # æ¦‚è¦½æª”æ¡ˆåŸºç¤ tokens
        }

        for topic_data in topics[:3]:  # æœ€å¤šè¼‰å…¥ 3 å€‹ä¸»é¡Œ
            topic_files = [f"layer2_topics/{topic_data['category']}/{f}" for f in topic_data["files"]]

            plan["topics"].append(
                {
                    "topic": topic_data["topic"],
                    "files": topic_files,
                    "tags": topic_data["tags"],
                    "related": topic_data.get("related", []),
                }
            )

            # æ¯å€‹ä¸»é¡Œæª”æ¡ˆä¼°ç®— 5K tokens
            plan["estimated_tokens"] += 5000 * len(topic_files)

        return plan

    def execute_loading_plan(self, plan: Dict) -> Dict:
        """
        åŸ·è¡Œè¼‰å…¥è¨ˆç•«

        Returns:
            {
                "loaded_files": ["chinese_overview.md", "tangshi.md"],
                "tokens_consumed": 7500,
                "content_preview": {...}
            }
        """
        self.loaded_files = []
        self.token_count = 0

        result = {
            "loaded_files": [],
            "tokens_consumed": 0,
            "content_preview": {},
        }

        # è¼‰å…¥æ¦‚è¦½
        if plan["overview"]:
            content = self.load_file(plan["overview"])
            if content:
                result["content_preview"]["overview"] = content[:500]  # å‰ 500 å­—å…ƒé è¦½

        # è¼‰å…¥ä¸»é¡Œæª”æ¡ˆ
        for topic_data in plan["topics"]:
            for file_path in topic_data["files"]:
                content = self.load_file(file_path)
                if content:
                    result["content_preview"][topic_data["topic"]] = content[:500]

        result["loaded_files"] = self.loaded_files
        result["tokens_consumed"] = self.token_count

        return result

    def log_performance(self, question: str, plan: Dict, result: Dict):
        """è¨˜éŒ„æ•ˆèƒ½ä¾›æ—¥å¾Œå„ªåŒ–"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "category": plan["category"],
            "confidence": plan.get("confidence", 0),
            "files_loaded": result["loaded_files"],
            "tokens_consumed": result["tokens_consumed"],
            "estimated_tokens": plan["estimated_tokens"],
        }

        log_file = self.skill_dir / "meta" / "performance_log.jsonl"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")


def main():
    """å‘½ä»¤åˆ—ä»‹é¢"""
    import argparse

    parser = argparse.ArgumentParser(description="è‡ºç£å­¸æ¸¬æ™ºæ…§çŸ¥è­˜è¼‰å…¥å™¨")
    parser.add_argument("--question", "-q", type=str, help="é¡Œç›®å…§å®¹")
    parser.add_argument("--topic", "-t", type=str, help="ç›´æ¥æŒ‡å®šä¸»é¡Œ")
    parser.add_argument("--load-details", action="store_true", help="è¼‰å…¥ç´°ç¯€æª”æ¡ˆ")
    parser.add_argument("--preview", action="store_true", help="åƒ…é è¦½è¼‰å…¥è¨ˆç•«")

    args = parser.parse_args()

    loader = SmartLoader()

    if args.question:
        print(f"ğŸ“ é¡Œç›®ï¼š{args.question}\n")

        # ç”Ÿæˆè¼‰å…¥è¨ˆç•«
        plan = loader.get_loading_plan(args.question)

        print("ğŸ“Š è¼‰å…¥è¨ˆç•«ï¼š")
        print(f"  åˆ†é¡ï¼š{plan['category']}")
        print(f"  ä¿¡å¿ƒåº¦ï¼š{plan.get('confidence', 0):.2f}")
        print(f"  ä¼°ç®— Tokensï¼š{plan['estimated_tokens']}")
        print(f"  ä¸»é¡Œæ•¸é‡ï¼š{len(plan['topics'])}")

        if plan["topics"]:
            print("\nğŸ¯ ç›¸é—œä¸»é¡Œï¼š")
            for i, topic in enumerate(plan["topics"], 1):
                print(f"  {i}. {topic['topic']}")
                print(f"     æ¨™ç±¤ï¼š{', '.join(topic['tags'][:3])}")
                print(f"     æª”æ¡ˆï¼š{topic['files'][0].split('/')[-1]}")

        if not args.preview:
            print("\nâ³ åŸ·è¡Œè¼‰å…¥...")
            result = loader.execute_loading_plan(plan)

            print(f"\nâœ… è¼‰å…¥å®Œæˆ")
            print(f"  å¯¦éš› Tokensï¼š{result['tokens_consumed']}")
            print(f"  å·²è¼‰å…¥æª”æ¡ˆï¼š{len(result['loaded_files'])}")

            # è¨˜éŒ„æ•ˆèƒ½
            loader.log_performance(args.question, plan, result)

    elif args.topic:
        topics = loader.find_topics(args.topic)

        if topics:
            print(f"ğŸ” æ‰¾åˆ° {len(topics)} å€‹ç›¸é—œä¸»é¡Œï¼š\n")
            for topic in topics:
                print(f"ä¸»é¡Œï¼š{topic['topic']}")
                print(f"åˆ†é¡ï¼š{topic['category']}")
                print(f"æ¨™ç±¤ï¼š{', '.join(topic['tags'])}")
                print(f"æª”æ¡ˆï¼š{', '.join(topic['files'])}")
                print(f"ç›¸é—œï¼š{', '.join(topic.get('related', []))}")
                print()
        else:
            print("âŒ æ‰¾ä¸åˆ°ç›¸é—œä¸»é¡Œ")

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
